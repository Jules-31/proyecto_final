{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbc5d13",
   "metadata": {},
   "source": [
    "# Clasificador de Instrumentos Musicales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Asegura el uso de la GPU si est谩 disponible\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b38f26",
   "metadata": {},
   "source": [
    "## Configuraci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e5b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci贸n\n",
    "SAMPLE_RATE = 22050\n",
    "MAX_LEN = 3  # segundos\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-4\n",
    "DATA_DIR = r\"C:\\Users\\saray\\Downloads\\oyopf\\all-sample-des\"  # Cambiar seg煤n ruta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5e280",
   "metadata": {},
   "source": [
    "## Procesamiento de Audio\n",
    "Clase que transforma archivos de audio a espectrogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add71ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Procesamiento de Audio\n",
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Clase encargada del procesamiento de audio crudo (forma de onda)\n",
    "    convirti茅ndolo en un espectrograma de Mel normalizado en decibelios.\n",
    "\n",
    "    Este procesamiento es com煤nmente utilizado como entrada para redes\n",
    "    neuronales en tareas de clasificaci贸n o detecci贸n de eventos de audio.\n",
    "\n",
    "    Atributos:\n",
    "        sample_rate (int): Frecuencia de muestreo esperada de los audios.\n",
    "        n_mels (int): N煤mero de bandas de Mel a generar.\n",
    "        mel_transform (MelSpectrogram): Transformador de onda a Mel.\n",
    "        db_transform (AmplitudeToDB): Convertidor de amplitud a decibelios.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=22050, n_mels=128):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        #Transformar ondas a un espectograma\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048, #Fourier para an谩lisis\n",
    "            hop_length=512, #Pasos\n",
    "            n_mels=n_mels,\n",
    "            power=2\n",
    "        )\n",
    "        #Amplitudes a escala logar铆tmica de decibelios\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "    def process(self, waveform, sample_rate):\n",
    "        \"\"\"\n",
    "        Procesa una forma de onda: la re-muestrea, normaliza y transforma\n",
    "        en un espectrograma de Mel logar铆tmico y normalizado.\n",
    "\n",
    "        Args:\n",
    "            waveform (Tensor): Tensor de forma [canales, muestras].\n",
    "            sample_rate (int): Frecuencia de muestreo original del audio.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Espectrograma de Mel de forma [1, n_mels, tiempo].\n",
    "        \"\"\"\n",
    "        #Poner todo en una frecuencia\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        #Cambiar a monoaudio\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        #Definir una duraci贸n    \n",
    "        target_samples = int(self.sample_rate * MAX_LEN)\n",
    "        if waveform.shape[1] > target_samples:\n",
    "            waveform = waveform[:, :target_samples]\n",
    "        else:\n",
    "            pad_amount = target_samples - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad_amount))\n",
    "        \n",
    "        waveform = waveform / waveform.abs().max()\n",
    "        mel = self.mel_transform(waveform)\n",
    "        mel_db = self.db_transform(mel)\n",
    "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-8)\n",
    "        return mel_db\n",
    "\n",
    "# 2. Dataset\n",
    "class InstrumentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para audios de instrumentos musicales,\n",
    "    organizados por carpetas (una por clase).\n",
    "\n",
    "    Carga archivos .mp3 o .wav, los transforma a espectrogramas de Mel,\n",
    "    y opcionalmente aplica aumento de datos.\n",
    "\n",
    "    Atributos:\n",
    "        processor (AudioProcessor): Instancia para procesar los audios.\n",
    "        augment (bool): Si se debe aplicar aumento de datos o no.\n",
    "        samples (list): Lista de tuplas (ruta_audio, etiqueta).\n",
    "        label_map (dict): Mapeo de 铆ndice a nombre de clase.\n",
    "        inverse_map (dict): Mapeo de nombre a 铆ndice de clase.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, augment=False, max_samples_per_class=200):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset escaneando las carpetas de clases.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Ruta al directorio que contiene subcarpetas\n",
    "                            (cada una representa una clase).\n",
    "            augment (bool): Si se debe aplicar aumento de datos.\n",
    "            max_samples_per_class (int): N煤mero m谩ximo de archivos por clase.\n",
    "        \"\"\"\n",
    "        self.processor = AudioProcessor(SAMPLE_RATE)\n",
    "        self.augment = augment\n",
    "        self.samples = []\n",
    "        self.label_map = {}\n",
    "        self.inverse_map = {}\n",
    "        \n",
    "        #Analizar subcarpetas\n",
    "        class_dirs = sorted(glob(os.path.join(data_dir, \"*\")))\n",
    "        for class_idx, class_dir in enumerate(class_dirs):\n",
    "            class_name = os.path.basename(class_dir)\n",
    "            self.label_map[class_idx] = class_name\n",
    "            self.inverse_map[class_name] = class_idx\n",
    "            \n",
    "            #Para que lea mp3 o wav\n",
    "            audio_files = glob(os.path.join(class_dir, \"*.mp3\")) + glob(os.path.join(class_dir, \"*.wav\"))\n",
    "            audio_files = audio_files[:max_samples_per_class]\n",
    "            \n",
    "            for audio_file in tqdm(audio_files, desc=f\"Cargando {class_name}\"):\n",
    "                self.samples.append((audio_file, class_idx))\n",
    "        \n",
    "        print(f\"\\nDataset cargado: {len(self.samples)} muestras, {len(self.label_map)} clases\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Devuelve una muestra procesada lista para ser alimentada a un modelo.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ndice de la muestra.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, int]: (espectrograma, etiqueta)\n",
    "        \"\"\"\n",
    "        audio_path, label = self.samples[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            spec = self.processor.process(waveform, sample_rate)\n",
    "            #Aumentar datos para entrenar\n",
    "            if self.augment and np.random.random() < 0.5:\n",
    "                spec = torch.roll(spec, shifts=np.random.randint(-10, 10), dims=2)\n",
    "            #Asegurar las dimensiones\n",
    "            if spec.dim() == 2:\n",
    "                spec = spec.unsqueeze(0)\n",
    "            return spec, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {audio_path}: {str(e)}\")\n",
    "            #Si hay errores, el espectograma es blanco\n",
    "            dummy = torch.zeros((1, self.processor.n_mels, int(SAMPLE_RATE * MAX_LEN / 512) + 1))\n",
    "            return dummy, 0\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        \"\"\"\n",
    "        Calcula pesos por clase inversamente proporcionales a su frecuencia.\n",
    "        Esto permite balancear clases desequilibradas durante el entrenamiento.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Pesos normalizados por clase (float32).\n",
    "        \"\"\"\n",
    "        counts = np.bincount([label for _, label in self.samples])\n",
    "        counts = np.where(counts == 0, 1, counts)\n",
    "        weights = 1. / counts\n",
    "        weights = weights / weights.sum() * len(weights)\n",
    "        return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "# 3. Modelo CNN\n",
    "class InstrumentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo simple de red neuronal convolucional (CNN) para clasificaci贸n\n",
    "    de espectrogramas de Mel.\n",
    "\n",
    "    Arquitectura:\n",
    "        - 3 bloques Conv2D + BatchNorm + ReLU + MaxPool\n",
    "        - Flatten + Linear\n",
    "\n",
    "    Atributos:\n",
    "        cnn_layers (Sequential): Capas convolucionales\n",
    "        fc (Linear): Capa final de clasificaci贸n\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagaci贸n hacia adelante.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Tensor de entrada [batch_size, 1, n_mels, tiempo].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Logits por clase.\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044a81b",
   "metadata": {},
   "source": [
    "## Visualizaci贸n del Entrenamiento\n",
    "Matriz y gr谩ficas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94a7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualizaci贸n\n",
    "class TrainingVisualizer:\n",
    "    \"\"\"\n",
    "    Clase para visualizar el entrenamiento del modelo, incluyendo:\n",
    "\n",
    "    - Evoluci贸n de la p茅rdida (loss) y precisi贸n (accuracy).\n",
    "    - Matriz de confusi贸n.\n",
    "    - Reporte de clasificaci贸n.\n",
    "\n",
    "    Args:\n",
    "        label_map (dict): Diccionario con el mapeo de 铆ndices a nombres de clases.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "    def update(self, epoch, tr_loss, val_loss, tr_acc, val_acc, model, val_loader):\n",
    "        \"\"\"\n",
    "        Actualiza las m茅tricas y genera visualizaciones cada 5 茅pocas o al final.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): N煤mero de 茅poca actual.\n",
    "            tr_loss (float): P茅rdida del conjunto de entrenamiento.\n",
    "            val_loss (float): P茅rdida del conjunto de validaci贸n.\n",
    "            tr_acc (float): Precisi贸n en entrenamiento.\n",
    "            val_acc (float): Precisi贸n en validaci贸n.\n",
    "            model (nn.Module): Modelo entrenado.\n",
    "            val_loader (DataLoader): Dataloader de validaci贸n.\n",
    "        \"\"\"\n",
    "        self.train_loss.append(tr_loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "        self.train_acc.append(tr_acc)\n",
    "        self.val_acc.append(val_acc)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "            self._plot_metrics()\n",
    "            self._plot_confusion_matrix(model, val_loader)\n",
    "    \n",
    "    def _plot_metrics(self):\n",
    "        \"\"\"Genera y guarda un gr谩fico de la evoluci贸n de p茅rdida y precisi贸n.\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss, label='Entrenamiento')\n",
    "        plt.plot(self.val_loss, label='Validaci贸n')\n",
    "        plt.title('Evoluci贸n de la P茅rdida')\n",
    "        plt.xlabel('poca')\n",
    "        plt.ylabel('P茅rdida')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.train_acc, label='Entrenamiento')\n",
    "        plt.plot(self.val_acc, label='Validaci贸n')\n",
    "        plt.title('Evoluci贸n de la Precisi贸n')\n",
    "        plt.xlabel('poca')\n",
    "        plt.ylabel('Precisi贸n')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_confusion_matrix(self, model, loader):\n",
    "        \"\"\"\n",
    "        Genera y guarda la matriz de confusi贸n junto con un reporte de clasificaci贸n.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): Modelo entrenado.\n",
    "            loader (DataLoader): Loader del conjunto de validaci贸n/test.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=self.label_map.values(),\n",
    "                    yticklabels=self.label_map.values())\n",
    "        plt.title('Matriz de Confusi贸n')\n",
    "        plt.xlabel('Predicciones')\n",
    "        plt.ylabel('Valores Verdaderos')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nReporte de Clasificaci贸n:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=self.label_map.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e3386",
   "metadata": {},
   "source": [
    "## Funciones de Entrenamiento y Validaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcdaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Funciones de Entrenamiento\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, label_map):\n",
    "    \"\"\"\n",
    "    Entrena el modelo CNN y guarda los mejores pesos seg煤n la precisi贸n en validaci贸n.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        train_loader (DataLoader): Datos de entrenamiento.\n",
    "        val_loader (DataLoader): Datos de validaci贸n.\n",
    "        criterion (Loss): Funci贸n de p茅rdida.\n",
    "        optimizer (Optimizer): Optimizador.\n",
    "        scheduler (LRScheduler): Planificador de tasa de aprendizaje.\n",
    "        num_epochs (int): N煤mero de 茅pocas.\n",
    "        label_map (dict): Mapeo de clases para visualizaci贸n.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Modelo entrenado con los mejores pesos.\n",
    "    \"\"\"\n",
    "    visualizer = TrainingVisualizer(label_map)\n",
    "    best_acc = 0.0\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        #Validar\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        avg_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_time = avg_time * (num_epochs - epoch - 1)\n",
    "        \n",
    "        visualizer.update(epoch, train_loss, val_loss, train_acc.item(), val_acc.item(), model, val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | '\n",
    "              f'Tiempo: {epoch_time:.1f}s | '\n",
    "              f'ETA: {remaining_time/60:.1f}min')\n",
    "    total_time = sum(epoch_times)\n",
    "    avg_epoch_time = total_time / num_epochs\n",
    "    print(f'\\nEntrenamiento completado en {total_time/60:.1f} minutos')\n",
    "    print(f'Tiempo promedio por 茅poca: {avg_epoch_time:.1f} segundos')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_dataloaders(data_dir, batch_size=32, val_split=0.2):\n",
    "    full_dataset = InstrumentDataset(data_dir, augment=True)\n",
    "    \n",
    "    indices = list(range(len(full_dataset)))\n",
    "    labels = [full_dataset.samples[i][1] for i in indices]\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=val_split, stratify=labels)\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    \n",
    "    train_labels = [full_dataset.samples[i][1] for i in train_indices]\n",
    "    class_weights = full_dataset.get_class_weights()\n",
    "    sample_weights = class_weights[train_labels]\n",
    "    \n",
    "    if (sample_weights <= 0).any():\n",
    "        sample_weights = torch.clamp(sample_weights, min=1e-8)\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, full_dataset.label_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d6825",
   "metadata": {},
   "source": [
    "## Nuevos Audios\n",
    "No lo he probado bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8c102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Clasificaci贸n de Audios Nuevos\n",
    "def predict_audio(file_path, model_path='best_model.pth', threshold=0.6, show_spectrogram=True):\n",
    "    \"\"\"\n",
    "    Clasifica un archivo de audio y visualiza el espectrograma y la probabilidad por clase.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo de audio (.wav).\n",
    "        model_path (str): Ruta al modelo entrenado.\n",
    "        threshold (float): Umbral de confianza para la predicci贸n.\n",
    "        show_spectrogram (bool): Si se desea mostrar el espectrograma.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, float]: Clase predicha y confianza asociada.\n",
    "    \"\"\"\n",
    "    if not hasattr(predict_audio, 'label_map'):\n",
    "        _, _, predict_audio.label_map = create_dataloaders(DATA_DIR, BATCH_SIZE)\n",
    "        predict_audio.model = InstrumentCNN(len(predict_audio.label_map)).to(DEVICE)\n",
    "        predict_audio.model.load_state_dict(torch.load(model_path))\n",
    "        predict_audio.model.eval()\n",
    "        predict_audio.processor = AudioProcessor(SAMPLE_RATE)\n",
    "    \n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        spec = predict_audio.processor.process(waveform, sample_rate)\n",
    "        spec = spec.unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = predict_audio.model(spec)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            conf = conf.item()\n",
    "            pred_class = predict_audio.label_map[pred.item()]\n",
    "        \n",
    "        print(f\"\\n Audio analizado: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        if conf >= threshold:\n",
    "            print(f\" Predicci贸n: {pred_class} (Confianza: {conf:.2%})\")\n",
    "        else:\n",
    "            print(f\" Predicci贸n incierta: {pred_class} (Confianza: {conf:.2%} < {threshold:.0%})\")\n",
    "        \n",
    "        print(\"\\nDistribuci贸n de probabilidades:\")\n",
    "        for i, prob in enumerate(probs.squeeze().cpu().numpy()):\n",
    "            print(f\"- {predict_audio.label_map[i]}: {prob:.2%}\")\n",
    "        \n",
    "        if show_spectrogram:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.imshow(spec.squeeze().cpu().numpy(), aspect='auto', origin='lower')\n",
    "            plt.title(f\"Espectrograma | Pred: {pred_class} ({conf:.2%})\")\n",
    "            plt.colorbar()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return pred_class, conf\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el audio: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378c1ab",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a1dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEN ===\n",
      "1. Entrenar modelo\n",
      "2. Clasificar un audio\n",
      "3. Salir\n",
      "Opci贸n no v谩lida.\n"
     ]
    }
   ],
   "source": [
    "# 7. Funci贸n Principal y Men煤\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta el pipeline completo de entrenamiento.\n",
    "    \"\"\"\n",
    "    print(\"=== CLASIFICADOR DE INSTRUMENTOS MUSICALES ===\")\n",
    "    print(f\"Dispositivo: {DEVICE}\")\n",
    "    \n",
    "    print(\"\\nCargando dataset...\")\n",
    "    train_loader, val_loader, label_map = create_dataloaders(DATA_DIR, BATCH_SIZE)\n",
    "    \n",
    "    model = InstrumentCNN(len(label_map)).to(DEVICE)\n",
    "    print(f\"\\nModelo creado con {len(label_map)} clases\")\n",
    "    print(f\"Total par谩metros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, label_map)\n",
    "    \n",
    "    print(\"\\nEntrenamiento completado!\")\n",
    "    print(f\"Mejores pesos guardados en: best_model.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== MEN ===\")\n",
    "    print(\"1. Entrenar modelo\")\n",
    "    print(\"2. Clasificar un audio\")\n",
    "    print(\"3. Salir\")\n",
    "\n",
    "    choice = input(\"Selecciona una opci贸n: \")\n",
    "    if choice == \"1\":\n",
    "        main()\n",
    "    elif choice == \"2\":\n",
    "        path = input(\"Ruta del archivo de audio: \")\n",
    "        if os.path.exists(path):\n",
    "            predict_audio(path)\n",
    "        else:\n",
    "            print(\" El archivo no existe. Verifica la ruta.\")\n",
    "    elif choice == \"3\":\n",
    "        print(\"隆Hasta luego!\")\n",
    "    else:\n",
    "        print(\"Opci贸n no v谩lida.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
